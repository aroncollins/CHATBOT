{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd512460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d68f4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b588fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\bill\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc0a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://twitter.com/search?q=%22Uhuru%20and%20Raila%22&src=trend_click&vertical=trends\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d42b76c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Define a sample text passage and associated questions and answers\n",
    "text = \"Steve Jobs was an American entrepreneur and businessman who co-founded Apple Inc. He was born in San Francisco, California in 1955 and was adopted at birth by Paul and Clara Jobs. Jobs attended Reed College in Portland, Oregon but dropped out after six months. In 1976, he co-founded Apple with Steve Wozniak and they introduced the first Apple computer that same year.\"\n",
    "\n",
    "questions = [\"Who co-founded Apple Inc?\", \n",
    "             \"Where was Steve Jobs born?\",\n",
    "             \"When was Steve Jobs born?\",\n",
    "             \"What college did Steve Jobs attend?\",\n",
    "             \"Who did Steve Jobs co-found Apple with?\",\n",
    "             \"What was the first product introduced by Apple?\"]\n",
    "\n",
    "answers = [\"Steve Jobs\",\n",
    "           \"San Francisco, California\",\n",
    "           \"1955\",\n",
    "           \"Reed College in Portland, Oregon\",\n",
    "           \"Steve Wozniak\",\n",
    "           \"The first Apple computer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80145083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text passage\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "clean_sentences = []\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in stop_words and word.isalnum()]\n",
    "    clean_sentence = ' '.join(words)\n",
    "    clean_sentences.append(clean_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c27da7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract named entities from the text passage\n",
    "tagged_sentences = [pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
    "\n",
    "named_entities = []\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    tree = ne_chunk(tagged_sentence)\n",
    "    for chunk in tree:\n",
    "        if hasattr(chunk, 'label') and chunk.label() == 'NE':\n",
    "            named_entities.append(' '.join(c[0] for c in chunk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03578aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a machine learning model to predict answers to questions\n",
    "def get_answer(question):\n",
    "    # Preprocess the question\n",
    "    words = word_tokenize(question)\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in stop_words and word.isalnum()]\n",
    "    clean_question = ' '.join(words)\n",
    "\n",
    "    # Extract named entities from the question\n",
    "    tagged_question = pos_tag(word_tokenize(clean_question))\n",
    "    tree = ne_chunk(tagged_question)\n",
    "    question_entities = []\n",
    "    for chunk in tree:\n",
    "        if hasattr(chunk, 'label') and chunk.label() == 'NE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d59d9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
